{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product2vec: Grocery Item Embeddings\n",
    "\n",
    "The original [Word2vec](https://arxiv.org/pdf/1301.3781.pdf) algorithm was released in 2013 and since then revolutionized the field of NLP. Simply put, word2vec learns high dimensional representations of words to convert one hot encoded features to more dense representations that can be used in algorithms.\n",
    "\n",
    "It enables computers to more easily do math on this representation and along the way can demonstrate some interesting mathematical operations using cosine similarity, showing things like the vector for *Queen* being very similar to the vector of *King* minus the word *male* vector plus the *female* vector.\n",
    "\n",
    "Products in a grocery store can be thought in a similar way to the bag of words approach used in text analytics, and there are some [interesting applications](https://arxiv.org/pdf/1810.08577.pdf) from NLP being ported over to the retail space.  \n",
    "\n",
    "The same way documents can be thought of being composed of a series of words, baskets can be thought of as composed of a series of items purchased. There is a notable difference, in that the context of words matters alot. The immediately adjacent words have a much larger impact on the meaning of a word then words far away from it. However in a grocery basket it's reasonable to think that the order items are scanned across a checkout does not necessarily have a large relevance, but rather their overall presence in a basket. \n",
    "\n",
    "There are plenty of [high quality data explorations](https://www.kaggle.com/philippsp/exploratory-analysis-instacart) available, so I'll be skipping that task. All customers having had 3 previous orders, and the task is on predicting reorders, the products should be ordered often enough that extremely rare products isn't a huge thing to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gage\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append our repeated orders with our prior orders, to get the complete t-log.\n",
    "orders = (pd.read_csv('data/order_products__train.csv')\n",
    "          .append(pd.read_csv('data/order_products__prior.csv'))\n",
    "         )\n",
    "\n",
    "#we'll use product_name strings\n",
    "products = pd.read_csv('data/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_cols = ['order_id','product_name']\n",
    "\n",
    "#downsample while I test the code for faster iteration on syntax. run full dataset before commit.\n",
    "sample_size = 1\n",
    "\n",
    "baskets = (orders\n",
    "           .merge(products,on='product_id',how='left')\n",
    "           .sample(frac=sample_size)\n",
    "          )[relevant_cols]\n",
    "\n",
    "#memory management on my local computer\n",
    "del([orders,products])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Organic Celery Hearts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Organic Whole String Cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cucumber Kirby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulgarian Yogurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Organic Hass Avocado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Lightly Smoked Sardines in Olive Oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384619</th>\n",
       "      <td>2</td>\n",
       "      <td>Garlic Powder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384625</th>\n",
       "      <td>2</td>\n",
       "      <td>Classic Blend Cole Slaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384620</th>\n",
       "      <td>2</td>\n",
       "      <td>Coconut Butter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384624</th>\n",
       "      <td>2</td>\n",
       "      <td>All Natural No Stir Creamy Almond Butter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384623</th>\n",
       "      <td>2</td>\n",
       "      <td>Original Unflavored Gelatine Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384622</th>\n",
       "      <td>2</td>\n",
       "      <td>Carrots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384618</th>\n",
       "      <td>2</td>\n",
       "      <td>Michigan Organic Kale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384621</th>\n",
       "      <td>2</td>\n",
       "      <td>Natural Sweetener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384617</th>\n",
       "      <td>2</td>\n",
       "      <td>Organic Egg Whites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384627</th>\n",
       "      <td>3</td>\n",
       "      <td>Unsweetened Almondmilk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384631</th>\n",
       "      <td>3</td>\n",
       "      <td>Organic Ginger Root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384633</th>\n",
       "      <td>3</td>\n",
       "      <td>Organic Ezekiel 49 Bread Cinnamon Raisin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         order_id                                   product_name\n",
       "5               1                         Bag of Organic Bananas\n",
       "2               1                          Organic Celery Hearts\n",
       "7               1                    Organic Whole String Cheese\n",
       "3               1                                 Cucumber Kirby\n",
       "1               1  Organic 4% Milk Fat Whole Milk Cottage Cheese\n",
       "0               1                               Bulgarian Yogurt\n",
       "6               1                           Organic Hass Avocado\n",
       "4               1           Lightly Smoked Sardines in Olive Oil\n",
       "1384619         2                                  Garlic Powder\n",
       "1384625         2                        Classic Blend Cole Slaw\n",
       "1384620         2                                 Coconut Butter\n",
       "1384624         2       All Natural No Stir Creamy Almond Butter\n",
       "1384623         2               Original Unflavored Gelatine Mix\n",
       "1384622         2                                        Carrots\n",
       "1384618         2                          Michigan Organic Kale\n",
       "1384621         2                              Natural Sweetener\n",
       "1384617         2                             Organic Egg Whites\n",
       "1384627         3                         Unsweetened Almondmilk\n",
       "1384631         3                            Organic Ginger Root\n",
       "1384633         3       Organic Ezekiel 49 Bread Cinnamon Raisin"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets.sort_values(['order_id']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Size\n",
    "\n",
    "This will matter as we use gensim's word2vec implementation for this task. The learning task for word2vec is predicting the a missing word given a window of words around it using a single hidden layer.  Rather than caring about the quality of the prediction, the weights of the hidden layer are what represent the product embedding that we will use. The number of neurons in the hidden layer is a tunable parameter. Unfortunately, there isn't great guidance on select for this, but eyeballing the resulting embeddings can give guidance on quality of fit. Some people recommend using the 4th root of unique tokens in our corpus, which I'll try.\n",
    "\n",
    "A tunable parameter for the algorithm is the context window, how many words around the target word to use for our prediction task. Given the lack of order, we will want to use the size of the largest basket, 145 for this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use vectors of length 14 for 49685 products\n",
      "The biggest basket (window in our algorithm) will be 145\n"
     ]
    }
   ],
   "source": [
    "num_items = baskets.product_name.nunique()\n",
    "embedding_size = np.floor(num_items**0.25).astype('int')\n",
    "print('''Let's use vectors of length {n} for {tokens} products'''.format(n=embedding_size, tokens = num_items))\n",
    "\n",
    "biggest_basket = np.max(baskets.groupby('order_id').product_name.nunique())\n",
    "print('''The biggest basket (window in our algorithm) will be {}'''.format(biggest_basket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaping our Data\n",
    "The gensim implementation of word2vec expects each document to be a list. Traditionally, each document is a list of words. In this case, each basket is a list of products. We will use the product name, which will be more expensive in memory but will make interpretation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_of_basket_lists = (baskets\n",
    "        .groupby('order_id')\n",
    "        .apply(lambda baskets :\n",
    "                baskets.product_name\n",
    "                .tolist()\n",
    "               )\n",
    "       )\n",
    "\n",
    "#memory management\n",
    "del(baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id\n",
       "1    [Cucumber Kirby, Organic Hass Avocado, Organic...\n",
       "2    [Coconut Butter, All Natural No Stir Creamy Al...\n",
       "3    [Lemons, Unsweetened Almondmilk, Unsweetened C...\n",
       "4    [Energy Drink, Oats & Chocolate Chewy Bars, Or...\n",
       "5    [Original Black Box Tablewater Cracker, Americ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_of_basket_lists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(df_of_basket_lists, size=embedding_size, window=biggest_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(word_u,word_v,model):\n",
    "    \"\"\"\n",
    "    Cosine similarity gets the similarity for two products and computes the similarity \n",
    "    between two embeddings in our word2vec model\n",
    "        \n",
    "    Arguments:\n",
    "        u - numpy array of shape (n,)        \n",
    "        v - numpy array of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine similarity between words u & v\n",
    "    \"\"\"\n",
    "    #get embeddings from gensim model\n",
    "    u = model.wv[word_u]\n",
    "    v = model.wv[word_v]\n",
    "\n",
    "    #compute similarity\n",
    "    dot = np.dot(u, v)\n",
    "    norm_u = np.sqrt(np.sum(u * u))\n",
    "    norm_v = np.sqrt(np.sum(v * v))\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945737"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a pair of similar identity items\n",
    "cosine_similarity('Organic Whole Milk','Organic Reduced Fat Milk',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38942885"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a pair of very different items\n",
    "cosine_similarity('Bag of Organic Bananas','Party Tumblers',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39023426"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a pair of similar items within a department\n",
    "cosine_similarity('Bag of Organic Bananas','Limes',model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "It's a bit handwavy to select a few cases where your embeddings work and call it a great job.  There is no real source of truth in this task however, product hierarchies like what department or aisle products are a part of are directionally accurate but not a base truth. \n",
    "\n",
    "Building the simple vector math to learn representations of features like *organic* are of interest. Showing that Organic apples - apples + steak equals organic beef would help demonstrate we are learning a real representation of product features.\n",
    "\n",
    "Predicting item re-ordering as in the initial challenge, would likely be aided by the embedding of the item being reordered.\n",
    "\n",
    "Implementing this in Keras rather than leveraging the gensim implementation would also be a worthwile exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
